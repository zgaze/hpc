
# 线程同步
进程内的多个线程共享同一虚拟地址空间，每个线程都是一个独立的执行流，所以，多个线程对应多个执行流，多个执行流会竞争同一资源的情况，资源包括内存数据、打开的文件句柄、套接字等，如果不加以控制和协调，则有可能出现数据不一致，而这种数据不一致可能导致结果错误，甚至程序奔溃，需要努力避免。

并发编程的错误非常诡谲且难以定位，它总是隐藏在某个的角落，大多数时候，程序运转良好，等代码交付上线后，莫名其妙的出错，就像墨菲定律描述的那样：凡是可能出错，就一定会出错。

## 为什么需要多线程同步？
我们先用2个例子来描述，如果不做线程同步，程序会出现什么问题。

### 例子1
有一个货物售卖程序，全局变量int item_num记录某商品的数量，它被初始值为100（代表可售卖数量为100）。售卖函数检查剩余商品数，如果剩余商品数大于等于售卖数量，则扣除商品件数，并返回成功；否则返回失败，代码如下：

```
bool sell(int num) {
    if (item_num >= num) {
        item_num -= num;
        return true;
    }
    return false;
}
```

单线程下，sell函数被依次调用，程序运转良好，结果符合预期，但在多线程环境下，会出错。

为了理解上述代码行为，需要先了解一个基本事实：程序变量存放在内存中，对变量做加减，会先将变量加载（load）到通用寄存器，再执行算术运算更新（update）寄存器中的值，最后把寄存器的新值存入（store）内存位置。

根据上述事实，我们知道寄存器里会保存内存变量值的副本，对变量的加减乘除等运算直接作用于副本，而非变量内存位置。不过，如果对变量赋值，则会指令会接受一个内存位置作为操作数，从而指令会直接操作内存位置。

#### 多核并行
- 假设线程t1和线程t2，分别在core1和core2同时执行，t1执行sell(50), t2执行sell(100)。
- 2个线程代表2个执行（指令）流，如果2个执行流同时执行到if (item_num >= num)这一行判断语句。
- 内存位置保存的item_num的值会被分别加载到2个核心的寄存器，因为item的值为100，所以加载到寄存器后也都为100，2个线程的条件检查都顺利通过。
- 线程t1，执行减法运算item_num -= num，参数num为50，结果为100-50=50，更新寄存器。
- 线程t2，执行减法运算item_num -= num，参数num为100，结果为100-100=0，更新寄存器。
- 然后，t1和t2线程先后把各自寄存器里的新值store到内存，后一个线程的store操作会覆盖前值。
- 如果t1线程先store，内存中的item_num被修改为50，t2线程再执行store，内存中的item_num被覆盖，item_num值被替换为0。
- 如果t2先store，t1后store，则item_num的最终值为50。

无论哪种情况，结果都是错误的，我们只有100件商品，却超卖出150件。

#### 单核并发
- 如果程序在单CPU单Core的机器上运行，t1线程和t2线程并发（非并行）交错执行，因为只有一个CPU，所以同一时刻，只有一个线程在执行。
- 假设t1在CPU上执行，它把item_num(100) load到寄存器，判断通过(100 >= 100)。
- 这时候，发生线程调度（比如t1的时间配额耗尽），t2被调度到CPU上执行，然后t2依次完成load、check、compute和store操作。
- 然后t1又被调度到CPU上恢复执行，t1会直接用寄存器中的item_num副本（100），执行计算，item -= 50的结果为50，更新寄存器，所以t1线程执行后，新值50被store入item_num所在内存。
- 我们期望t1或者t2的sell只有一个操作成功，但结果并非如此。

让我们再看一个计数的例子：

### 例子2
全局变量int count用来计数，我们启动100个线程，每个线程的处理逻辑：在1000次循环里累加count；主函数启动线程并等待所有线程执行完成，程序退出前打印count数值，代码如下:

```
#include <thread>
#include <iostream>

int count = 0;

void thread_proc() {
    for (int i = 0; i < 1000; ++i) {
        ++count;
    }
}

int main() {
    std::thread threads[100];
    for (auto &x : threads) {
        x = std::move(std::thread(thread_proc));
    }
    for (auto &x : threads) {
        x.join();
    }
    std::cout << "count:" << count << std::endl;
    return 0;
}
```
我的机器上打印count:97424，而我们期望的结果是100*1000=100000，结果与预期不符，为什么会这样？

简单的一行++count，实际上也包括：从内存加载变量的值到寄存器（load），寄存器中更新值（update），将寄存器中的新值存入内存（store）。

因为多个线程并发/并行执行，而load/update/store的三步不是原子的，不能在一个cpu cycle内完成，所以多个线程可能加载了相同的旧值，在寄存器中分别更新，再写入变量count的内存位置，导致最终值比期望的10000小，而且运行多次，会出现不同的结果。

## 问题出在哪里？
上述2个例子都表明：不加控制的多线程并发/并行访问同一数据，会导致数据不一致，从而产生错误的结果，所以，需要某种同步机制，协调多线程的运行，确保结果的正确性。

上述问题的根因都出在多线程对数据的竞争访问，数据竞争不仅因为对数据的访问不能在一个指令周期内完成，也因为我们经常要先读一个变量值，再基于它的值做决策，而这2个步骤的组合并非原子的。

只有同时满足以下情况，才需要做多线程同步：
- 数据竞争是前提条件，多线程对同一数据的并发访问
- 所有线程对数据只读的话，没有问题，不需要同步，必须有线程对数据进行写（修改），多线程混合读写才需要同步
- 对数据的访问在时间上要同时或者交错（多线程对数据读写在开始结束时间有重叠），如果时间上能错开，也没有问题
- 出现的不一致数据结果，是不能忍受的错误（比如的超卖，计数错误），否则，也不必同步，这一点往往被忽略

## 保护的到底是什么？
- 首先，明确一点，多线程同步，保护的是数据，而非代码，代码保存在进程的文本段，程序运行过程中，只会从保存文本段的内存位置读取指令序列，而不会修改文本（代码）段。
- 其次，细化一点，我们保护的是全局资源/数据、static局部数据、或者通过指针/引用指向的堆内存上的数据；而普通局部变量则通常不需要保护，因为局部变量位于栈上，每个线程有独立的栈，线程栈是相互隔离的，不通过特殊手段不可互访。

## 多线程同步机制有哪些？
线程间同步的机制有很多，Posix线程同步机制包括互斥锁（Mutex）、读写锁（Reader-Writer Lock）、自旋锁（Spin Lock）、条件变量（Condition Variable）、屏障（Barrier）等。
C/C++、Java等编程语言都有类似的线程同步机制和编程接口，细节上不尽相同，但概念和原理上都是相通的，我们主要考查Posix线程同步机制。

### 互斥锁
互斥锁，某个线程获得锁之后，直到该线程释放锁之前，其他的线程便没法获得锁，表现出排他的特征。

互斥锁，本质上是做串行化，即在同一时间点，访问受保护数据的临界代码，只会有一个执行流正在运行，直到这段临界代码执行完，其他执行流才有机会进入这段临界代码，这就从根本上杜绝了某个数据被多个执行流交错访问。

互斥锁的用法，通常是在访问共享资源前加锁，完成资源访问后再解锁，当申请加锁的时候，锁已经被其他线程持有，那么申请加锁的线程便会阻塞住，当锁被释放（解锁），则阻塞在该锁上的所有线程，都会被唤醒，只有一个线程会加锁成功，其他线程意识到锁处于加锁状态，则又转入阻塞等待解锁，因此，一次只有一个线程会前进。用mutex对sell做同步的代码如下：

```
bool sell(int num) {
    mutex.lock();
    if (item_num >= num) {
        item_num -= num;
        mutex.unlock();
        return true;
    }
    mutex.unlock();
    return false;
}
```

加锁保护后，对item_num的判断和减值这2个行代码，将变成密不可分的原子操作，多线程同时调用sell()都不会出现前述的超卖错误。

**RAII**
上面的程序，有一个容易出错的点，那就是在两次返回的时候都需要解锁，如果函数有多个返回点，则需要在每个返回点都小心的解锁，如果逻辑更复杂一点，或者调用的其他函数抛出异常，则很难确保unlock得到正确的调用。

所以，C++提供一个叫RAII的技术，常用来应对这种情况，RAII利用C++临时对象的析构在对象出作用域时一定会得到调用的特性，来提升安全性，通过构建一个临时对象，在对象的构造函数里加锁，在析构函数里解锁，来完成配对的操作，常用于加/解锁，打开/关闭文件句柄，申请/释放资源等操作。

```
class LockGuard {
    std::mutex& mutex;
    LockGuard(const LockGuard&) = delete;
    LockGuard(LockGuard&&) = delete;
    LockGuard& operator=(const LockGuard&) = delete;
    LockGuard& operator=(LockGuard&&) = delete;
public:
    LockGuard(std::mutex& mutex) : mutex(mutex) { mutex.lock(); }
    ~LockGuard() { mutex.unlock(); }
};

int item_num = 100;
std::mutex item_num_mutex;

bool sell(int num) {
    LockGuard lg(item_num_mutex);
    if (item_num >= num) {
        item_num -= num;
        return true;
    }
    return false;
}
```

**注意**：
- 通过互斥锁对数据进行保护，需要开发者遵从约束，按某种规定的方式编写数据访问代码，这种约束是对程序员的隐式约束，它并非某种强制的限制。
- 比如使用互斥量对数据x进行并发访问控制，假设有2处代码对该数据竞争访问，其中一处用互斥量做了保护，而另外一处，没有使用互斥锁加以保护，则代码依然能通过编译，程序依然能运行，只是结果上可能是错误，这个跟现实中没有钥匙开锁就不能得到（访问）权限是不一样的。

### 自旋锁
在cpu核上自旋，粘着cpu不停的测试，直到其他cpu解锁，这是一种消耗cpu的加锁方式，适合持有锁的时间非常短的情况，它基于一种假设，睡眠导致的调度开销大于在cpu上自旋测试的开销。

### 读写锁
写排他，读并行，适合多读少写的场景，可以提升吞吐。

### 条件变量（生产者-消费者模式）
需要配合互斥量使用，常用于生产者消费者模式

假设你要编写一个网络处理程序，I/O线程从套接字接收字节流，反序列化后产生一个个消息（自定义协议），然后投递到一个消息队列，一组工作线程负责从消息队列取出并处理消息。

这是典型的生产者-消费者模式，I/O线程生产消息（往队列put），Work线程消费消息（从队列get），I/O线程和Work线程并发访问消息队列，显然，消息队列是竞争资源，需要同步。

我们可以给队列配置互斥锁，put和get操作都先加锁，操作完成再解锁。代码差不多是这样的：

```
void io_thread() {
    while (1) {
        select(max_fd, read_fds, write_fds, nullptr, ...);
        Msg* msg = read_msg_from_socket();
        msg_queue_mutex.lock();
        msg_queue.put(msg);
        msg_queue_mutex.unlock();
    }
}

void work_thread() {
    while (1) {
        msg_queue_mutex.lock();
        Msg* msg = msg_queue.get();
        msg_queue_mutex.unlock();
        if (msg != nullptr) {
            process(msg);
        }
    }
}

```

work线程组的每个线程都忙于检查消息队列是否有消息，如果有消息就取一个出来，然后处理消息，如果没有消息就在循环里不停检查，这样的话，即使负载很轻，但work_thread还是会消耗大量的CPU时间，我们当然可以在两次查询之间加入短暂的sleep，从而让出cpu，但是这个睡眠的时间设置为多少合适呢？设置长了的话，会出现消息到来得不到及时处理，设置太短了，还是无辜消耗了CPU资源，这种不断问询的方式在编程上叫轮询。

轮询行为逻辑上，相当于你在等一个投递到楼下小邮局的包裹，你下楼问过没到之后，就上楼去，然后马上又下楼问，你不停的上楼下楼+询问，其实，你大可不必如此，何不等包裹到达以后，让门卫打电话通知你呢？

条件变量提供了一种类似通知的机制，它让两类线程能够在一个点交汇。条件变量能够让线程等待某个条件发生，条件本身受互斥锁保护，因此条件变量必须搭配互斥锁使用，线程在改变条件前先获得锁，然后改变条件状态，再解锁，然后发出通知，等待条件的睡眠中的线程在被唤醒前，必须先获得锁，再判断条件状态，如果条件不成立，则继续转入睡眠并释放锁。

对应到上面的例子，工作线程等待的条件是消息队列非空，用条件变量改写上面的代码：

```

void io_thread() {
    while (1) {
        select(max_fd, read_fds, write_fds, nullptr, ...);
        Msg* msg = read_msg_from_socket();
        {
            std::lock_guard<std::mutex> lock(msg_queue_mutex);
            msg_queue.push_back(msg);
        }
        msg_queue_not_empty.notify_all();
    }
}

void work_thread() {
    while (1) {
        Msg* msg = nullptr;
        {
            std::unique_lock<std::mutex> lock(msg_queue_mutex);
            msg_queue_not_empty.wait(lock, []{ return !msg_queue.empty(); });
            msg = msg_queue.get();
        }
        process(msg);
    }

}

```
lock_guard是互斥量的一个RAII包装类，unique_lock除了会在析构函数自动解锁外，还支持主动unlock调用。

生产者在往msg_queue投递消息的时候，需要对msg_queue加锁，通知work线程的代码可以放在解锁之后，等待msg_queue_not_empty条件必须受msg_queue_mutex保护，wait的第二个参数是一个lambda表达式，因为会有多个work线程被唤醒，线程被唤醒后，会重新获得锁，检查条件，如果不成立，则再次睡眠，条件变量的使用必须非常谨慎，否则容易出现不能唤醒的情况。

Posix条件变量的编程接口跟C++的类似，概念上是一致的。

### 原子变量&原子操作
C++提供一种类型为atomic<>的类模板，它提供++/--/+=/-=/fetch_sub/fetch_add等原子操作。

原子操作，从语义上理解，既一系列操作是密不可分的整体，要么都做，要么都不做，不会做一半，理解到这，使用就没什么问题了。

#### lock-free
下面是用atomic实现的一个免锁堆栈:

```
template <typename T>
struct node {
    T data;
    node* next;
    node(const T& data) : data(data), next(nullptr) {}
};
 
template <typename T>
class stack {
    std::atomic<node<T>*> head;
public:
    void push(const T& data) {
      node<T>* new_node = new node<T>(data);
      new_node->next = head.load(std::memory_order_relaxed);
      while(!head.compare_exchange_weak(new_node->next, new_node,
                                        std::memory_order_release,
                                        std::memory_order_relaxed))
          ; // the body of the loop is empty
    }
};
```

### 内存屏障

## 相关概念
### 线程安全
不访问全局变量，没有static local变量，只影响参数，无副作用

### 线程可重入

### 锁的粒度
锁的粒度太大，会影响并发，导致性能下降，但锁的粒度太大，会增加代码复杂度，需要平衡。

### 锁的范围
尽量减少锁的范围，不必锁覆盖的代码范围越小越好。

### 线程私有

### 死锁
死锁常见于两种情况。

#### ABBA锁
假设程序中有2个资源X和Y，分别被锁A和B保护，线程1持有锁A后，想要访问资源Y，而访问资源Y之前需要申请锁B，而如果线程2正持有锁B，并想要访问资源X，为了访问资源X，所以线程2需要申请锁A。

线程1和线程2分别持有锁A和B，并都希望申请对方持有的锁，因为线程申请对方持有的锁，得不到满足，所以便会陷入等待，也就没有机会释放自己持有的锁，对方执行流也就没有办法继续前进，导致相持不下，无限互等，导致死锁。

上述的情况似乎很明显，但如果代码量很大，有时候，这种死锁的逻辑不会这么浅显，它被复杂的调用逻辑所掩盖，但抽茧剥丝，最根本的原因就是上面描述的那样。

我们描述了死锁发生的典型场景，这种情况叫ABBA锁，既某个线程持有A锁申请B锁，而另一个线程持有B锁申请A锁。这种情况可以通过try lock实现，尝试获取锁，如果不成功，则释放自己持有的锁，而不一根筋下去。

#### 自死锁
对于不自持重复加锁的锁，如果线程持有某个锁，而后又再次申请锁，因为该锁已经被自己持有，再次申请锁必然得不到满足，从而导致死锁。

## 这种情况需要加锁吗？

## 扩展知识
### 分布式锁
