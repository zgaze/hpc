# 内存管理和优化

- 存储程序原理奠定了现代计算机的基础，将程序像数据一样存储在计算机内存，计算机一条接着一条指令执行。内存不仅存储数据，而且存储程序，是计算机系统的核心功能组件之一。
- 围绕局部性原理，计算机系统构建起“寄存器->缓存(SRAM)->内存(DRAM)->磁盘(HDD/SSD)”的存储层次结构：离CPU越近，容量越小、速度越快、单元价格越贵，离CPU越远，容量越大，速度越慢，单元价格越便宜。
- 内存可被为磁盘的高速缓存，因为CPU的速度比磁盘快很多，所以，通过在CPU与磁盘之间架设内存这座桥梁，填平了CPU和磁盘之间的速度鸿沟。
- 内存管理和优化对性能影响很大，是编写高性能程序的关键，但内存管理和优化牵扯很多系统底层知识，本章将梳理相关内容，从软硬件结合出发，力求简明扼要讲清楚这个主题的内容，如需更深入的理解，则需扩展阅读。

## CACHE
- 为什么需要CACHE？局部性原理
- 三级CACHE结构，L1-L2 CPU内core共享，L3 CPU独立
- 内存和CACHE的关系
- cache对性能的影响
	- 访问延迟对比
	- CacheLine
	- Cache miss / Cache hit，颠簸
- 分析和测量CACHE命中率
- 编写CACHE友好代码

## 虚拟存储
为什么一个进程所需的存储空间大小能超过物理内存的大小？操作系统是如何管理机器上运行的多个进程的内存的？进程间共享存储是如何做到的？通过top命令查看的VIRT和RES指标有什么不同？所有这些问题都跟虚拟存储这个概念相关，虚拟存储是计算机系统的重要概念，可以说，理解好虚拟存储便是掌握了内存管理的钥匙。

- 进程是执行中的程序，一个进程就是一个执行中的程序实例，同一个程序可以有多个执行的实例，对应多个进程。
- 系统上同时运行的多个进程共享机器的CPU和存储资源，每个进程（线程）有一个独立的逻辑执行流，它提供一种假象，好像在独占的使用处理器；同时，每个进程有一个私有地址空间，这提供另一个假象，它好像在独占的使用存储器。
- 虚拟存储是一层抽象，它为每个进程提供了一致的、私有的地址空间，借助这一层关键抽象，计算机科学中最深刻最成功的概念“进程”才得以实施。
- 虚拟存储简化了存储器管理，链接器以独立于内存物理地址的方式构建可执行程序。

---
虚拟存储空间被分为内核空间和用户空间两部分，并非所有虚拟地址空间都会分配物理内存，只有实际使用的才会分配物理内存，这也体现在通过top命令查看进程的VIRT和RES两项指标的数值差异上。

- 物理内存可视为一个巨大的字节数组，每个元素占用一个字节，有自己的独立编号（也就是地址），这个地址叫物理地址（Physical Address, PA）。
- 物理内存资源被运行在系统中的所有进程共享，就像CPU资源被运行在系统中的所有进程/线程共享一样。
- 应用程序中所使用的地址叫虚拟地址（Virtual Address, VA），每个进程都拥有独立的私有虚拟地址空间，进程之间的虚拟存储是隔离的，既进程A不通过特殊手段无法访问进程B的某个虚拟地址。
- 虚拟地址在访问真正的物理内存的时候，需要被转换为物理地址, 虚拟地址到物理地址的转换过程叫地址翻译。
- CPU硬件和操作系统合作完成地址翻译，CPU芯片上的存储管理单元（MMU）查询内存中的页表动态完成地址翻译，而页表的内容由操作系统管理维护，这项工作由底层系统默默完成，对应用程序透明。
- 进程X和进程Y中的同一虚拟地址会被映射到不同物理地址，多个进程也可以通过共享存储技术（Shared Memory）映射到同一块物理内存。

---
- 32位系统的虚拟地址范围是[0-2^32]，总共4G Byte，这个地址的集合叫地址空间，64位系统拥有更大的地址空间，但不是2^64。
- 虽然进程拥有如此大的虚拟地址空间，但它通常不会真正占用这么大存储空间。
- 系统以页为单位管理存储，32位系统上，PageSize通常为4K字节，64位系统上，PageSize通常为8K字节。
- 物理存储器（内存）被以PageSize为单位分割为物理页（Physical Page, PP），[0-4096)的连续空间被分为第1页，[4096,8192)的连续空间被分为第2页，以此类推，PageSize大小的物理页也被称为页帧。
- 虚拟存储空间也被按同样的PageSize分割成虚拟页（Virtual Page, VP），虚拟页分为已分配和未分配两种状态，而已分配的页又被细分为未缓存和已缓存两种状态。
- 进程的所有已分配的虚拟存储页构成进程的有效虚拟存储空间，对未分配的虚拟存储空间的访问非法，将触发异常（例如常见的段错误）
- 通过调用malloc/new/mmap等编程接口分配虚拟存储页，对物理内存页的分配由系统负责。
- 分页后，一个虚拟地址被分为2部分：页编号 + 页内偏移
- 操作系统在内存中为每个进程维护一个页表（PageTable, PT）, 地址翻译硬件通过查询存放在物理存储器中的页表，来找到对应的物理地址。
- 每个虚拟页对应一个页表条目（Page Table Entry, PTE），页表视为页表条目的数组，页号就是数组下标（索引）。
- 每个页表条目包含该虚拟页是否已分配，对未分配的页的访问非法，如果已分配，则又要区分是否已缓存（对应到物理内存页）和未缓存。
- 如果已缓存（页命中），则页表条目包含该虚拟页对应的物理地址；如果未缓存（页命失），则页表条目包含该虚拟页对应磁盘上该虚拟页的起始位置，系统在物理存储器中挑选一个牺牲页，并将虚拟页从磁盘拷贝到内存，这个过程叫换页（swapping）
- 牺牲页的内容需要从内存拷贝到磁盘虚拟页，这个过程叫换出（swap out），被缓存的新页需要从磁盘拷贝到内存，这个过程叫换入（swap in），因为牵扯到磁盘操作，过程中，一直等待，成本很高，这个成本被称为命失惩罚，但根据局部性原理，程序经历启动阶段的初始后，通常只会在一个较小的活动页面集上工作，这便能保证，虽然惩罚的成本看似很高，但实际上，它依然工作的很好。
- 通过页面调度，我们的程序能够在超过物理内存容量的虚拟存储空间下工作，且多个进程间，能有效的共享稀缺的内存资源。
- 理解这个过程对掌握内存管理和优化技术至关重要。

### linux进程虚拟存储空间
- linux进程的虚拟存储空间自底向上分为：代码段、全局变量段、堆、共享存储区、栈、内核段。
- 程序启动时，加载器会将编译后的可执行程序文件中的.text节拷贝到代码段，.data拷贝到全局变量段，每个函数（内联除外）编译后都会占存储空间，进文本节，程序执行中，会从代码段源源不断的加载指令。
- 堆向上生长，brk指向堆顶，通过malloc/new等编程接口从堆分配内存，动态分配的内存块需要手动释放。
- 栈向下生长，局部变量位于栈中，函数调用时的参数也经栈传递，函数调用链所需内存由栈提供，栈是自动伸缩的，每个线程会有独立的栈，每个栈的空间有限（4/8M，可调节），所以不能局部变量不能过大，递归过深有爆栈分险。
- 堆内存和栈内存本质上都是存储区，位于进程的不同区段，只有使用方式上的不同，没有物理介质上的不同，都会经地址翻译到物理内存。
- 栈和堆之间是共享存储区，通过共享存储编程接口shmget创建的存储段位于该段，标准库的代码在进程间也被共享，这样能够节省内存。
- 内核段存储空间，用户态代码不可访问，但用户代码调用系统调用、或者触发异常，进程陷入内核，会执行内核段的代码+访问内核态数据。
- 注意：代码段并非从0开始，而是从特定地址开始，0x8048000（32位系统），0x400000（64位系统）

### 段页式内存管理
- 早期计算机系统，采用段页式的内存管理，既有分页，又有分段，段内包含页，但linux系统简化了这个管理方式，进程的虚拟地址空间只有1段，所以相当于变相的废弃了分段。
- 如果以4K为一页，那么32位系统，虚拟内存空间为4G（2^32），因为页表是进程私有，所以，一个进程的虚拟地址空间包含1M页，需要1M页表项（64位更多），而系统中经常成百上千个进程，这个内存占用量太大了，所以，实际上，操作系统使用多级页表巧妙的解决了这个问题。
- 多级页表是压缩页表内存占用的惯用法，引入多级页表后，顶级页表的一个表项不再表示4K/8K的地址范围，它大的多，只有一级表项表示的范围被分配，其对应的2级页表才会被存储，所以极大的节省了内存空间，而因为进程实际分配的虚拟页，只是整个地址空间的很小一部分，所以，我们得以以小的存储代价，支撑很大的地址范围。
- 每次地址翻译，都需要查询内存页表，如果页表条目不在缓存中，则开销很大，为了加快内地翻译速度，便引入了TLB（翻译后备缓存），TLB是MMU中的一个PTE的小的缓存，MMU在走地址翻译的时候，先从TLB查找PTE，失败了再去PageTable里找，还是因为局部性，TLB极大的加速了地址翻译的过程。

---
- 软硬件协作
    - 地址翻译，需要操作系统、MMU（TLB）协作完成
    - 操作系统为每个进程维护页表
    - MMU先查TLB缓存，没找到，再查页表
    - 进程调度后，页表基址寄存器会修改指向新的运行进程的页表其实地址
- page fault：
    - page fault：实际上并不是真正的错误，没有名字看起来这么严重，指令执行时，引发缺页（page fault）会触发异常，操作系统的异常处理程序会妥善处置这个异常，并再次发射这个指令，这时候，因为请求的地址已经被装载到内存，指令得以正常进行。
    - major page fault: 也叫hard page fault，major page fault主要是由swapping机制引入的，因为牵扯到磁盘io，主要由软件完成，所以速度较慢，可通过swapon/swapoff开关系统交换分区，也可以设置修改系统设置：swappiness
    - minor page fault：也叫soft page fault，指需要访问的代码/数据已经在物理内存页中，但页表中的映射还没有建立，需要MMU建立物理内存和虚拟地址空间的映射关系。当一个进程在调用malloc获取虚拟空间地址后，首次访问该地址会发生一次soft page fault。多个进程访问同一个共享内存中的数据，当某些进程还没有建立起映射关系，访问时也会出现soft page fault。
    - 可以通过命令：ps -eo min\_flt,maj\_flt,cmd 查看系统各个进程的page fault情况。

- segmentation fault

- 当程序通过malloc分配1G字节的内存时，系统并不会为进程分配1G的物理内存，而只是分配1G的虚拟内存页，当之后真正访问某个页的时候，系统才会为它分配物理内存，如果系统物理内存被耗尽，则会挑选一个内存页，交换出去（把页的内容写入磁盘页），通过这样的策略，我们能获得比物理内存更大的存储空间，提高了内存资源的利用率，也使得进程之间共享存储变得可能。

- 相关linux命令：pmap、vmstat、free，虚拟内存占用和RES，getrusage

## 动态内存分配

### 动态内存分配器的历史
如果你在上世纪80年代，为unix系统编写上层应用程序，你可能无法使用动态内存分配器，因为它还不存在。你可能难以想象在脱离动态内存分配器的环境中编写程序，但实际上，它是可能的。

你要动态分配一块内存，通常是因为要到程序实际运行时才知道要申请多大的空间，比如你编写一个拼接两个字符串的函数，拼接后的结果串长度依赖于调用函数时候传递的2个参数字符串，怎么办？你可以用全局变量的方式预先申请一块全局的内存空间（就像用汇编编写程序那样），为了保险起见，你只能把这个全局存储空间预留得大一点，这种硬编码通常不是一个好主意，预留大了浪费存储空间，预留小了又不够用，但至少这样做是可行的，多个函数甚至可以复用这块空间，只要访问时间上能错开。

你也可以通过mmap系统调用申请一块内存空间，用完后用munmap解除映射。虽然mmap/munmap不如malloc/free编程接口那么快捷和高效，但它也是能正常工作的。

那为什么我们还需要动态内存分配器呢？因为虽然可以通过mmap/kmalloc这样的系统调用动态分配内存，但系统调用要陷入内核，每次动态分配内存都要陷入内核，这样的话，效率太低了，用户态的事情尽量用户态解决。

一个直观的想法：既然底层系统调用的代价高，那可不可以先申请一块存储空间，然后在用户态管理这些空间，来满足用户态对动态内存的请求，动态内存分配器通过调用mmap/sbrk等low-level调用向操作系统申请内存。这种批发转零售的思想就是动态内存分配器的设计动机，通过引入中间层，用高效的用户态调用，对抗高昂的内核态调用开销。

dlmalloc是一个著名且流行的动态内存分配器，由Doug Lea于1987年开始编写，dlmalloc就由Doug Lea名字首字母缩写而来，Doug Lea不仅开发了dlmalloc，同时也是并发编程领域的大神，是Java util.concurrent包的作者。

dlmalloc的实现只有一个源文件，虽然代码量只有5000行（且注释占了大量篇幅），但它却是一个高品质的佳作。

Doug Lea在代码里使用了大量的技巧，虽然时至今日，dlmalloc的一些技术已经显得有些落伍，但许多更先进更高效的内存分配器都从dlmalloc精巧的设计中获得了启发和灵感。

glibc默认的动态内存分配器ptmalloc就是基于dlmalloc开发的，ptmalloc通过引入arena的概念，把堆内存分割为多个不同的空间，从而减少了多线程竞争，解决了dlmalloc并发下工作的低效问题。

前面讲到，动态内存分配器引入了一个新的分层，这一层介于操作系统和应用程序之间，它的角色很神奇，因为在kernel看来，动态内存分配器依然是用户层代码，它运行在用户态，而在动态内存分配器的用户看来，它却像系统底层，它不是一般意义上的用户层代码。

当调用malloc的时候，动态内存分配器不一定会向操作系统申请内存，当调用free的时候，内存也不一定真正返还系统，这是因为动态内存分配器托管了这部分已分配的内存，用户层对动态内存的malloc请求可能从缓存得到满足，用户层主动free的内存块也有可能被动态内存分配器缓存起来，何时真正返还给系统，取决于动态内存分配器的实现策略，所以，用户层的malloc/free可能导致从os层面看到的进程的虚拟内存占用情况不变。

动态内存分配器如今早已成为标准C库的一部分，我们可以在任何支持C的开发环境下放心使用它。但有了动态内存分配器这一层，我们依然可以直接调用mmap动态申请内存，它铺在kernel之上，为应用程序开发提供服务，却没有完全屏蔽掉kernel的内存分配相关接口。

内存分配器有两种风格，一种是C/C++这种需要手动显式释放内存块，这种叫显式分配器；另一种是JAVA这种由分配器追踪被分配的内存块，在确定其不再被使用后主动将其回收，这种也叫隐式分配器，隐式分配器也叫垃圾收集器，自动释放不再被使用的内存块的过程叫垃圾收集。

隐式分配器的实现思路主要是通过引用计数技术追踪内存块被使用情况，增加一个引用就增加计数，减少一个引用就减少计数，再计数降到零的时候，便意味着该内存块不再被使用，可以释放了。

C++智能指针的实现技术跟隐式分配器的思路差不多，只不过C++智能指针是库级别的支持，这表示C++的智能指针是可选项，你可以用，也可以不用，而JAVA等是语言级别的支持，它从根本上统一管理。根据引用计数的放置的不同，又分为侵入式的引用计数（intrusive_ptr）和外置式的引用计数（shared_ptr）。

隐式分配器看起来更省事，也更安全，那为什么还需要显式的内存分配器呢？天下没有免费的午餐，你需要为GC付出额外的代价，而显式内存分配器，应用程序可以选择释放的时机，可以更精细化管理内存。

### C动态内存分配器接口
- malloc(size_t size)：分配内存块
    - 接受一个size参数，返回可用内存空间不小于size内存块的指针。glibc的ptmalloc，可以通过size_t malloc_usable_size(void* p)查询到实际可用尺寸（可能大于size），但建议你不要占这个小便宜，注意这个接口不是标准接口，它是产品相关的，你换了一个内存配置器就不一定能用
    - 返回的内存地址会做内存对齐，32位系统8字节对齐（低3位为0），64位系统16字节对齐（低4位为0）
    - size传0是合法的
- free(void* p)：释放内存块
    - free接受的参数，必须是malloc/calloc/realloc等接口返回的指针
    - free(nullptr)是合法的

### 动态内存分配器的目标
动态内存分配器要满足通用目的，即对申请的内存块的size，不应有假设，用户可能一次申请小到几个字节的内存块，也有可能一次申请上G的内存块。

另外，分配/释放的行为也各不相同，比如在申请/释放操作频繁交替进行，也有可能会先集中申请，再集中释放，只要满足释放时传入的块是之前分配返回的即可。

动态内存分配器对应用程序不应有假设，要能胜任各种场景，而上层的变化和差异极大，要在所有场景，各种情况下，有较好的综合表现，因而挑战很大，这也给场景定制的内存分配器、内存池技术发挥的舞台。

- 我们先看一看动态内存分配器的主要目标
    - 高吞吐：动态内存分配器要有高的吞吐，吞吐被定义为时间单位内满足分配/释放的次数。
    - 低延迟：从提出内存分配申请，到返回结果的时间间隔，越短越好，要立即响应分配请求
	- 高内存利用率：进程的虚拟内存空间受限于磁盘交换空间的限制，虚拟存储是有限的，需要高效利用起来，有效载荷要高，碎片要少，overhead小

### 内存碎片：内碎片 / 外碎片
内存碎片是指虽然未使用内存尺寸大于请求的内存尺寸，但因为没有完整的单块内存能满足分配请求的情况，碎片是造成堆内存利用率低的元凶，内存碎片分为内碎片和外碎片。

内碎片指分配块的实际内存大于有效载荷造成的，比如通过malloc(1)分配1字节的内存块，实际占用的内存会远大于1字节，这是因为虽然你只申请1字节，实际上动态内存分配器分配的可用内存会比1字节大，你可以通过malloc_usable_size(void* p)验证这一点；此外，内存对齐也会造成内存浪费；而且，内存分配器在会每个内存块增加头部、脚部，这也会需要额外的内存。

外碎片是指内存块与块之间的间隙，比如内存块A与B之间有5字节的间隙，内存B与C之间有3个字节间隙，这时候，需要分配8字节内存，虽然AB和BC之间的间隙加起来满足8字节的要求，但因为他们不连续，所以也没有办法利用起来。

### 动态内存管理器要处理哪些问题？
从动态内存分配器的接口malloc/free可知，动态内存分配器主要是一次从堆内存分一大块内存，然后再从这块大的内存块，分割出小块分配出去，最简单的做法就是在为这个大块内存记录一下位置（游标），这个位置初始化为大块内存的起始地址，然后下次分配的时候，先判断这个大块内存是否足够，如果不够，则再分一块大的，如果够，则将游标往后移动需要的字节数，返回游标移动前的值，下次分配则从新的游标位置开始，这样是最快的，其实很多内存池也是这么做的，当然，实际上，我们不能简单移动申请字节数大小，需要做对齐，但这影响我们的讨论。

但这有一个问题，那就是我们分配出去的块，怎么释放，释放后的块应该被复用，而且临近的块被释放，应该合并成更大的块，所以，我们要把free掉的块管理起来，比如用free list管理起来，可能还需要按size组织成不同的free list，实际上ptmalloc正是这么做的。

如果不同size有不同的free list，那释放内存块的时候，因为释放接口（free）只接受一个void*参数，那怎么获得这个内存块的size呢？实际上藏在块的头部，malloc返回的时候，并非直接返回块的其实地址，在起始的几个字节，会被用来作为头部，头部里面保存有块的size，还有其他一些信息，用来辅助做前述的空闲块合并。

实际上，内存块，不光有头部，还有脚部，头尾部用来记载一些有用的信息，这就不难理解为什么malloc一个字节，实际上消耗的内存远大于1字节。

动态内存分配器，会维护多个不同size的free list，但不能为所有size维护单独的free list，因为size的范围实在太大了，于是，只会选择2^n这种固定size，这样，当你申请20字节的时候，它会帮你向上取到32字节，然后去32字节的fixed-size freelist去找内存块，如果找不到，那就去64字节的freelist借用一个过来，再劈成2半，剩下的32字节块加入fixed-size为32的free list，如果还是找不到，就通过sbrk去调堆顶指针，获得一块更大的虚拟内存，实际实现远比这个复杂精巧，比如对huge size会直接mmap一块，比如还需要考虑到多线程的分配效率问题，但主要的思路差不多是这样了。

- 一次malloc要耗费多少cpu时间
- 几种经典的内存分配器：ptmalloc / tcmalloc / jemalloc
- 动态内存分配器的挑战（引出为什么需要MemPool）

## 内存池

### 为什么内存池更快？(因为放弃了中间free，本质上是以空间换时间)
- 池化/池技术是一种通过复用，从而降低运行时开销的通用技术，池的本质是空间换时间。
- 系统启动时，按设定预留资源；运行中，向池动态申请资源；用完后，将资源返还池。
- 池技术能减少资源创建和回收的开销，提升系统的并发和吞吐，广泛应用于低延迟系统。

既然系统已经有了动态内存分配器，而且新的动态内存分配器层出不穷，比如tcmalloc/jemalloc，为什么还需要内存池呢？

换言之，内存池为什么能做到比通用内存分配器快很多，究其根本，是因为通用内存分配器，要支持每个分配的块的独立回收，通过malloc分配的内存块，可以在程序运行中的任何时间，通过free返回系统，就这一个要求，决定了动态内存分配器快不到哪里去，它需要在释放的时候处理碎片合并，需要在分配的时候处理找空闲块以及多线程等问题，而池可以不用处理多线程问题，池可以只记录一个当前分配位置的游标，分配的时候，简单的移动游标即可，它不需要为分配的块分配额外头部，分配出去的块不会单独回收，这才是它快的秘诀。

- 批发转零售的策略
- 经典内存池的实现案例

## doris是如何做内存管理的？
Apache Doris是一个基于MPP架构的高性能、实时的分析型数据库，我们分析一下它的内存管理方案。

**doris的内存管理方案分三层：**
- 系统分配器（SystemAllocator）：封装系统/标准接口，提供统一的allocate/free接口
    - allocate(size)根据size调用posix_memalign()或者mmap()分配内存，会在分配的时候做内存对齐
    - free()接口调用munmap()或者free()释放内存
    - mmap()/munmap()配对，posix_memalign()/free()配对，通过配置二选一
- 块分配器（ChunkAllocator）
    - 块（Chunk）：Chunk表示通过SystemAllocator::allocate接口分配的内存块，Chunk包含内存块首地址、尺寸、core_id等信息
    - 为每个CPU core维护一个chunk_arena，chunk_arena类似ptmalloc里arena的概念，不同的是ptmalloc中的arena对应到线程，每个线程一个arena
    - 每个chunk_arena包含一个chunk_list的数组
    - chunk_list为每个size维护一个该size的chunk集合，为了减少各种size的数量，只维护固定size的chunk集合，比如8、16、32、64、128、256...，所以如果分配请求的大小是34字节，那么会向上圆整到64字节，通过size求对数就得到该size的块所属chunk_list（在chunk_list数组中的下标）
    - 块分配器会使用上述的系统配置器分配/回收内存，块配置器是单件（唯一实例），分配Chunk的接口是线程安全的
- 内存池（MemPool）
    - 提供allocate()、clear()、free_all()等操作接口
    - 维护通过allocate接口分配的ChunkInfo的列表，ChunkInfo在Chunk上增加了一个已分配字节数
    - 内存池会通过块分配器分配大块，每次分配的大块的大小会按X2（策略决定）增加，从而确保不会频繁调用块分配器的allocate接口
    - 通过内存池的allocate接口分配的内存，不支持单块free，不支持中途free，只支持统一释放free_all()
    - 内存复用：clear()接口会重置ChunkInfo上Chunk的已分配字节数，clear并不会真正回收内存
    - 内存池的操作接口是非线程安全的
---
### SystemAllocator
屏蔽了动态内存管理相关的底层系统调用和标准C/Posix编程接口，上层应用不再直接调用底层接口，而是调用SystemAllocator封装的编程接口：allocate/free。

- ChunkAllocator是怎么工作的？
    - ChunkAllocator处于3层结构的中间层
    - ChunkAllocator在SystemAllocator之上，会使用SystemAllocator的allocate/free接口申请和释放内存块
    - ChunkAllocator在MemPool之下，提供allocate和free接口供MemPool使用
    - ChunkAllocator减少了多线程竞争，ChunkAllocator维护core_num个ChunkArena对象，ChunkArena内维护一个chunk_list数组，为size=2^n的固定size块维护一个free list，内存申请的时候，会对请求的size向上圆整

---
- 因为每个core都有一个ChunkArena对象，所以上层应用代码申请内存的时候，先获取当前线程正在哪个核上运行，从而找到对应的ChunkArena对象，再通过size找到对应的free list，再从该free list上摘除一个块，返回给提出内存申请的上层。

- 多个逻辑线程依然可能调度到同一个核上执行，虽然多个线程不会在一个核上同时执行申请动态内存，但多个线程在一个核上交错执行（申请内存）的情况，依然会引发对free list的数据竞争（虽然这种情况出现的概率很小），这时候只需要用test_and_swap原子操作不停尝试就行了，如果尝试一定次数还不成功，则执行线程主动yield，让出CPU，从而让另一个在该核上执行内存分配的线程有机会继续执行，进而修改atomic_flag，然后之前yield CPU的线程被重新调度执行。

- TAS（test and swap）是很快的，且冲突概率变得非常小（因为每个核都有一个atomic_flag，不会所有线程竞争一个锁），这样的免锁设计，让分配内存变得很高效。

- ChunkAllocator也做了一层cache，通过ChunkAllocator::free释放的内存块，并不一定会真正调用底层的free，只在预留size超过配额（2G）的情况下，才会调用SystemAllocator的free()，这样进一步减少了对系统底层动态内存管理相关API的调用频次。

ChunkAllocator是单件，唯一实例，被所有MemPool对象共享。

---
### MemPool

- 咱们进一部分分析MemPool的设计，先给一张MemPool的图：

首先，我们来看MemPool的作用：

- 内存池在SystemAllocator/ChunkAllocator/MemPool的层次结构中，位于顶层，它依赖于下层ChunkAllocator，间接依赖SystemAllocator，下层的类不反向依赖于MemPool。

- 先说Chunk和ChunkInfo。

    - Chunk就是底层接口单次分配的内存块，Chunk持有内存块首地址data，内存块大小size，以及分配的时候执行线程在哪个core上执行。

    - ChunkInfo包含Chunk，同时多了一个int allocated_size，这是因为，为了减少对SystemAllocator::allocate()的调用次数，所以单次分配的chunk会比较大，几K，几十K，甚至XX M（兆），这个大的size记录在chunk->size上，但是，上层应用一次分配的内存可能比较小，几十字节之类，所以，该chunk还有多少字节可用（已经使用了多少字节），需要有一个记录，这就是allocated_size，相当于一个游标，每次从该chunk分配x字节，那就把allocated_size这个游标往增长的方向移动x字节（实际上会考虑到对齐）。


- 所以，对SystemAllocator::allocate()的调用，相当于批发进货；对MemPool::allocate()的调用，相当于零售。效果上，就是减少了底层API的调用频率，减少了多线程竞争。

- MemPool持有一个next_chunk_size，它表示下次调用ChunkAllocator分配接口allocator的时候，需要分配多大，它被初始化为4K，下次分配的时候，会增加到8K，当然如果下次申请的size大于8K，则会取max。

- next_chunk_size会一直增加，直到触达最大配置值，这样的设计，目的还是为了减少底层分配次数。

- 每次ChunkAllocator::allocate()都会返回一个Chunk，进而包装为ChunkInfo，被MemPool管理起来，所以MemPool会有多个ChunkInfo，用chunk_index标识chunk。

- MemPool记录一个current_chunk_idx，这个idx记录了上次成功分配的ChunkInfo，下次分配的时候，先从current_chunk_idx指向的chunkInfo里尝试分配，如果该ChunkInfo的剩余内存空间不够，则会查找其他ChunkInfo，直到找到能满足分配请求的ChunkInfo，如果现有的所有ChunkInfo都不满足，那就走ChunkAllocator的allocate，并把新申请的Chunk，放入ChunkInfo list。

- MemPool不支持单次分配的内存free，但是支持free_all，这会free该MemPool的所有Chunk。

- MemPool::Clear()接口不会真正free Chunk，而是会重置allocated_size，复用原内存chunk。

- 一个细节，关于ChunkAllocator，分配的时候，会首先从线程运行的core上的ChunkArena分配，如果没有合适的，会从其他Core的ChunkArena里分配，再分配不到，才会从system_allocate，这样做的目的，是减少内存cache量。

我们做内存池有几个目标：

- 吞吐，吞吐越大越好，能满足各种不同size，各种内存分配场景的大吞吐最好。
- 提高存储空间利用率，千方百计减少碎片（内碎片+外碎片）。

为了提高速度，我们经常要做cache，但是cache多了，会造成宝贵的内存资源的浪费，所以，需要balance。

---
	- Nginx内存池
	- loki小对象分配器
    - 延伸：对象池

## 内存泄漏
何谓内存泄漏？动态申请的内存丢失引用
- C++ operator new/delete重载
- C wrap malloc/free
- 地址消毒器
- valgrind/cachegrind/asan

## 其他

- mmap
- Shared Memory
- 内存对齐及影响：posix\_memalign 
- 对内存的优化不应该作为项目的高优先级目标

## 参考资料

